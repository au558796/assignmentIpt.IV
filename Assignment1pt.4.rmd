---
title: "Assignment 1 pt. 4"
author: "Dana Jensen"
date: "October 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this exercise we will assess how many participants we would need to adequately replicate our findings (ensuring our sample size is adequate, our alpha at 0.05 and our beta at 0.8).

### Exercise 1

How much power does your study have (if your model estimates are quite right)?
- Load your dataset, fit your favorite model, assess power for your main effects and interactions of interest.
- Report the power analysis and comment on what you can (or cannot) use its estimates for.
```{r}
#prelude
setwd("C:/Users/Dana/Desktop/METHODS III/assignmentIpt.IV")
library(pacman)
p_load(dplyr, ggplot2, lmerTest,simr, MASS)
train = read.csv("df_train.csv")

#Vogue
model = lmer(CHI_MLU ~ VISIT + Diagnosis +(1+VISIT | SUBJ),train, REML = FALSE)
summary(model)

#I've got the power
powerV = powerSim(model,fixed("VISIT"),nsim=10)
powerV #100%, 69.15 - 100
powerD = powerSim(model,fixed("Diagnosis"),nsim=10)
powerD #40%, 12.16 - 73.76

```
ANSWER:
I used the model of child mlu as the dependent variable with visit and diagnosis as predictor variables, with a different slope and intercept for each child (subject as random effect). Running the simulation 10 times on 61 participants with the fixed effect of visit produced a power of 100% with confident intervals of 69.15 and 100. Running a simulation with the same number of simulations and participants for the fixed effect of diagnosis produces a power of 40% with confidence intervals of 12.16 and 73.76. Doing power analyses are useful for a few reasons. First of all, if you are planning on running a study, it's an irreplaceable way to discover how many participants (or stimuli) you will need when running your study to ensure the final results can be meaningful. Second, if you have already conducted a study, a power analysis is useful for uncovering the magnitude of effect size that is possible with the amount of participants you have. Lastly, if wanting to compare two studies, or simply calculate how reliable a study's results are, you can calculate a studies power.

### Exercise 2

How would you perform a more conservative power analysis?
- Identify and justify a minimum effect size for each of your relevant effects
- Take the model from exercise 1 and replace the effects with the minimum effect size that you'd accept.
- Assess the power curve by Child.ID, identifying an ideal number of participants to estimate each effect
- OPTIONAL if your power estimates do not reach an acceptable threshold simulate additional participants and repeat the previous analysis
- Report the power analysis and comment on what you can (or cannot) use its estimates for.
```{r}
#visit = 0.23, Diagnosis = 0.29 normally
fixef(model)["VISIT"] <- 0.0695 # but are we interested in 0.0695??
pv = powerSim(model,fixed("VISIT"),nsim=100)
pv
#ERROR: DIAGNOSIS IS NOT THE NAME OF A FIXED EFFECT?????
fixef(model)["Diagnosis"] <- 0.30
powerCurveV = powerCurve(model, fixed("VISIT"),along="SUBJ", nsim=100)

```


```{r}


### Riccardo's clumsy function to simulate new participants
### TO DO points are only notes for myself, so not part of the assignment

createNewData <- function (participants,visits,model){
  # participants is the number of subjects
  # visits is the number of visits
  fe <- fixef(model)
  Intercept <- fe[1] #intercept
  bVisit <- fe[2] #visit
  bDiagnosis <- fe[3] #diagnosis
  bVisitDiagnosis <- fe[4] #visit diagnosis interaction
  vc<-VarCorr(model) # variance component
  sigmaSubject <- as.numeric(attr(vc[[1]],"stddev")[1]) # random intercept by subject
  sigmaVisit <- as.numeric(attr(vc[[1]],"stddev")[2]) # random slope of visit over subject
  sigmaResiduals <- as.numeric(attr(vc,"sc"))
  sigmaCorrelation <- as.numeric(attr(vc[[1]],"correlation")[2])
  # Create an empty dataframe
  d=expand.grid(VISIT=1:visits,SUBJ=1:participants)
  # Randomly sample from a binomial (to generate the diagnosis)
  condition <- sample(rep(0:1, participants/2))
  d$Diagnosis<-condition[d$SUBJ]
  d$Diagnosis[is.na(d$Diagnosis)]<-1
  ## Define variance covariance matrices:
  Sigma.u<-matrix(c(sigmaSubject^2,
                    sigmaCorrelation*sigmaSubject*sigmaVisit,
                    sigmaCorrelation*sigmaSubject*sigmaVisit,
                    sigmaVisit^2),nrow=2)
  ## generate new fake participants (column1=RandomIntercept, column2=RandomSlope)
  u<-mvrnorm(n=participants,
             mu=c(0,0),Sigma=cov(ranef(model)$SUBJ))
  ## now generate fake data:
  ### the outcome is extracted from a gaussian with
  ### the solution to the model's equation as mean and
  ### the residual standard deviation as standard deviation 
  d$CHI_MLU <- rnorm(participants*visits,
                     (Intercept+u[,1]) +
                     (bVisit+u[,2])*d$Visit + 
                     bDiagnosis*d$Diagnosis ,sigmaResiduals)  
  
  return(d)
}
```
### Exercise 3

Assume you have only the resources to collect 30 kids (15 with ASD and 15 TDs). Identify the power for each relevant effect and discuss whether it's worth to run the study and why.

```{r}
d<-createNewData(30,6,model)
model = lmer(CHI_MLU ~ VISIT + Diagnosis +(1+VISIT | SUBJ),d, REML = FALSE)
pd = powerSim(model,fixed("VISIT"),nsim=100)



```